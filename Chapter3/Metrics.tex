\subsection{Evaluation Metrics}

A key area of the work was deciding the specific metrics use to evaluate the performance of the models. Metrics are vital to determine if models were under or over-fitting on our data and helps to provide context into steps and modifications needed to improve the performances of our models. 
As a multi-class classification problem, we concerned on primarily two main metrics of evaluation: 


\subsubsection*{AUC-ROC}

The Area Under the Receiver Operating Characteristic Curve (AUC-ROC) measures the ability for a model to correctly distinguish between positive and negative classes. AUC-ROC is also insensitive to class imbalances. Similarly in the works carried in \parencite{s22155633};\parencite{pick_quality_over} AUC was used as one of the primary evaluation metrics.

\medskip

This value is first calculated by plotting the Receiver Operating Characteristic (ROC) curve using the True Positive Rate (TPR) against the False Positive Rate (FPR) for each classification thresholds. The TPR is measure of the proportions of positive values that were correctly classified. Similarly, the FPR is the proportion of negative values that are incorrectly classified as positive. Using the ROC curve, the area under the curve (AUC) is calculated. This value ranges between 0 and 1, where 0.5 represents at best random guessing, and 1 corresponds a perfect classifier.

\medskip

As our problem is multi-class, the AUC will be calculated by computing the one-vs-all metric for each class separately i.e,  calculated for each class individually, treating all samples for that class as positive and all other as negative. Then these scores are averaged to calculate a final AUC score.

\subsubsection*{F1}

The F1 score is a weighted average of both precision and recall. Precision is the fraction of correctly predicted positive instances out of all total predicted positive instances. Recall is the fraction of correctly predicted positive instances out of the total actual positive instances.

The F1-score was chosen due to its representation in an imbalanced dataset; as it considers both precision and recall. Accuracy can be a misleading metric %(\citealp[]{FAWCETT2006861}; \citealp[]{grandini2020metrics}). A model can predict the majority class i.e 'Normal' in most cases and still receive high accuracy, but in reality it poorly represents the minority classes.

\subsubsection*{Equations for Precision, Recall \& F1} 

\begin{equation*} Precision = \frac{True\ Positive}{True\ Positive + False\ Positive} \end{equation*}

\begin{equation*} Recall = \frac{True\ Positive}{True\ Positive + False\ Negative} \end{equation*}

\begin{equation*}
F_1 = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}
\end{equation*}

\subsubsection*{Micro, Macro and Weighted}

In regular binary classification, metrics such as F1, Precision, Recall and AUC can be calculated easily, however for our multi-class classification problem a slightly different approach must be taken. In particular, there are three main methods:

\begin{itemize}
    \item Micro averaging uses the metric across all classes by counting the total true positives, false positives, and false negatives. This is the equivalent of using the accuracy i.e, fails to take into account class imbalances.
    \item Macro averaging calculates the metric in each class independently and then averages this for all classes, giving equal weight for all classes. It is used typically when all classes are equally as important, irrespective of the class size or any imbalances.
    \item Weighted averaging also calculates the metric for each class independently, but the average of the individual class scores are weighted with the number of samples in each class. It is used when performance across all classes are considered important, and the class imbalance needs to be considered.
\end{itemize}

Therefore, the weighted averaging method was chosen, leading to robust scores that takes into account both the number of samples within the class and its performance. It was observed that most previous works fails to mention the averaging method used for its evaluation metrics.

\subsubsection*{Classification Report}

In addition to viewing the averaged metrics across all classes, the classification report provides a comprehensive summary of detailing the metrics for Precision, Recall, Accuracy and F1 across each class. 

\subsubsection*{Confusion Matrix}

The Confusion Matrix is a table that displays the performance of a model by showing the number of true positives, false positives, true negatives and false negatives for each class. In other words, how accurate the classifier is on each class and how it tends to wrong predict each class for another (confusion). By examining the confusion matrix, we can identify any specific classes that may require additional tuning or changes to the model to improve its performance. Works by \citeauthor{pmlr-v29-Koco13} introduced a new method using confusion matrices to measure and analyse the performance of cost-sensitive methods showing the importance of the confusion matrix in imbalanced data sets.
