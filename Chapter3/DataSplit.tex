\subsection{Dataset Manipulation}

The AWID3 Dataset is supplied in two formats, a set of CSV files representing each method of attack and its subsequent data and the raw PCAP network captures. For this instance, the CSV files were utilised, and the dataset was manipulated to suit the purpose of experimentation. Each attack contained a folder with the data split into multiple CSV files; these needed to be rejoined to form one file/dataset so that it could be utilised and processed accordingly. 

\medskip
The methodology proposed was as follows:
\begin{enumerate}
    \item Combine all individual CSV files for each attack method into one file using a bash script.
    \item Import the file as a data frame and extract the desired features into a separate data frame.
    \item Remove Nan and fix invalid values
    \item Replace missing values to 0
    \item Remove Nan target values.
    \item Export the data frame as a new CSV file.
    \item Combine all reduced datasets into one large data set.
\end{enumerate}

\medskip

\textbf{Combining Files}

\smallskip
A bash script, Appendix \ref{appx: CSV Combiner Script} was created to list all contents of a given folder, containing the \textit{.csv} file extension and sorted into numerical order, i.e. 01, 02, 03. However, each file contained the CSV header, so only the first CSV file's header was read and written into the new \textit{'combined.csv'} file. All other files were read and appended into the new file, ignoring the first line, the CSV header. 

This step resulted in 6 large CSV files with the following rows and file sizes. See Table \ref{tab:full_data}

\begin{table}[H]
\centering
\begin{tabular}{llllll}
\cline{1-3}
\textbf{Class}  & \textbf{Rows} & \textbf{File Size} &  &  &  \\ \cline{1-3}
SSH              & 2,440,571     & 3 GB               &  &  &  \\
Botnet           & 3,226,061     & 4.27GB             &  &  &  \\
Malware          & 2,312,761     & 3.41GB             &  &  &  \\
SQL Injection    & 2,598,357     & 3.8 GB             &  &  &  \\
SSDP             & 8,141,645     & 8.02 GB            &  &  &  \\
Website Spoofing & 2,668,568     & 2.85 GB            &  &  &  \\ \cline{1-3}
\end{tabular}
\caption{Data Before Cleaning and Processing}
\label{tab:full_data}
\end{table}

\medskip

\textbf{Feature Extraction}

\smallskip
With the combined datasets, the selected features were extracted from the 254 features as referenced in Table \ref{tab:application_features}, \ref{tab:802.11_features} and \ref{tab:non80211}. Due to the large file sizes, numerous errors and kernel crashes were encountered while importing the file into Pandas. 

Instead of importing all columns, the required features were specified using the \textit{'use\_cols'} parameter along with the \textit{'chunk size'} parameter to read the file in smaller chunks to save memory and eventually combined them, forming one data frame. This saw a reduction in import time and lower memory consumption.

\medskip
\textbf{Data Cleaning}

\smallskip
The data was cleaned to ensure it was fit for the next data pre-processing stage. Rows that contained only NAN values were dropped, as well as missing Label values. Each column's missing/nan values were replaced and represented with 0, following a similar approach to \textcite{s22155633}.

\smallskip
Upon analysis, frequent hyphened values were observed, e.g. \textit{-100-100-10, 123-456-1, -10-2, 81-63-63 etc.}. These were more notable in the 802.11w features such as \textit{'radiotap.dbm\_antsignal'} and \textit{'wlan\_radio.signal\_dbm'}, this was expected, as being wireless radio features, \textit{'radiotap.dbm\_antsignal'} represents the signal strength in decibel milliwatts (dBm) and is captured via multiple antennas each representing the captured signal strength. A similar approach to \parencite{s22155633} was followed, extracting and keeping the first value in the sequence, e.g. \textit{-100-100-10} became -100, \textit{123-456-1} became 123, \textit{-10-2} became -10 and \textit{81-63-63} became 81. A regex expression was written to iterate through each column to replace these values accordingly.

\smallskip
Following on, invalid values were observed, such as the presence of values containing months such as: \textit{Oct-26, Oct-18, Feb-10} etc. This was proposed to be a processing error during the creation of the CSV files from the PCAP files and represented a low majority of the dataset. It was concluded that rows containing invalid values would be dropped from the data. A similar RegEX expression was written to filter out these values from the following columns: \textit{'tcp.option\_len', 'dns.resp.ttl', 'ip.ttl', 'smb2.cmd'}. The complete code for this section can be found in Appendix \ref{appx: Feature Extraction}.

\medskip

\textbf{Individual Datasets}

\smallskip
After data cleaning and processing, the final six individual data files consisted of the following. See Table \ref{tab:reduced_data}

\begin{table}[H]
\centering
\begin{tabular}{llllll}
\cline{1-3}
\textbf{Class}  & \textbf{Rows} & \textbf{File Size} &  &  &  \\ \cline{1-3}
SSH              & 2,433,851    & 298 MB               &  &  &  \\
Botnet           & 3,216,505     & 393 MB             &  &  &  \\
Malware          & 2,304,632     & 283 MB             &  &  &  \\
SQL Injection    & 2,590,119     & 317 MB             &  &  &  \\
SSDP             & 8,137,106     & 1.04 GB            &  &  &  \\
Website Spoofing & 2,666,406     & 340 MB            &  &  &  \\ \cline{1-3}
\end{tabular}
\caption{Data After Cleaning and Processing}
\label{tab:reduced_data}
\end{table}

\medskip

\textbf{Combining Datasets}

\smallskip
Finally, utilising the same bash script (\ref{appx: CSV Combiner Script}), the six reduced CSV files were combined into one large single data frame and then subsequently exported to a CSV file. The resulting file was 2.67GB in size and contained approximately 21,348,614 rows. 