 %!TEX root =  ../Report.tex

\section{Experiments}
\label{sec: Experiments}

This project utilised a diverse set of machine-learning algorithms for this problem. Consisting of three shallow classifiers: Random Forest, K-Nearest Neighbour and XGBoost and one neural network: Multi-Layered Perceptron. The code for all models and experiments can be found within the codebase and Appendix \ref{appx: Model Code}.

\subsection{Initial Modelling}
To speed up initial training and testing for each machine learning algorithm, many subsets of the original combined data were created using sklearn's train\_test\_split to create a stratified split resulting in reduced datasets. Varying data splits were made, including a 50\%, 60\% and 80\% data split from the original ~12 million rows of data as seen in Table \ref{tab:split_data}. The reduced datasets allow for a quicker training time to determine each model's suitability and help provide a rough measure of a model's underlying data performance. As discussed previously, additional Cross Validation is used during training where appropriate.

\subsection{Parameter Tuning} 
An important aspect of experimentation is tuning parameters specific to each model. Hyperparameters are defined during the creation of each model and can substantially impact its performance. As such, two primary methods are adopted. Sometimes, an exhaustive searching method such as GridSearchCV is initially used to identify the optimal parameters. GridSearchCV uses a user-defined parameter grid and systematically evaluates each combination relative to the model's performance. However, this can be computationally expensive and time-consuming, so due to limitations in both hardware, time and numerous crashes, GridSearchCV was not always used. 

\smallskip

To address this issue, RandomizedSearchCV was adopted; by searching randomly through the parameter grid with a defined number of iterations, a good tradeoff is achieved that provides a balanced and efficient solution without sacrificing quality. Additionally, in some cases and initial experimentation, iterative experimentation and domain knowledge were used instead. This was justified due to limited time constraints and prior knowledge and understanding of the underlying algorithm. 

