%!TEX root =  ../Report.tex
\subsection{Classifiers}

%Classification on the combined set of features was conducted against three classifier models, Random Forest (RF), XGBoost and K-Nearest Neighbour (KNN).

\subsubsection{Random Forest}
\label{sec:RF}

In an attempt to find optimal parameters, GridSearchCV and RandomizedSearchCV were utilised. However, due to memory issues on both machines and frequent system crashes due to insufficient memory, experimentation for the Random Forest Classifier instead follows an exploratory approach, using prior knowledge and iterative testing to create a series of models that were subsequently evaluated and optimised. The training was conducted on both the M2 and VM testbeds. Table \ref{tab:rf-params} details the parameters focused on during experimentation, using performance feedback and the model's behaviour on the dataset; these were tweaked and fine-tuned to help enhance the model's performance. Table \ref{tab:rf-parameters} documents the parameters used for all models.

\medskip

Initial experimentation began using default parameters without changing or adding values (Model 0-2). This helped to establish the baseline performance for subsequent comparison and analysis. This initial model achieved high S-CV average scores on the entire dataset. With such high metrics, it was essential to ensure modifications made to the model avoided overfitting. 

\medskip

Subsequent experimentation was conducted using the model's training metrics, confusion matrices, classification reports and predictions on the test set. Models 3 and 5 noted that the modification of the \textit{class\_weight} parameter from its default value to \textit{'weighted'} resulted in a decrease in S-CV performance across all metrics. Moreover, inspecting the metrics on the test set affirms this, with a higher number of misclassifications but higher recall and very low precision rates for several classes. 

During the iterative and exploratory phases, it was observed that additional models and future parameter tuning, i.e. Models 3-5, all showed negative performance impacts compared to the baseline model. This indicated that contrary to expectations, using additional parameter values did not increase the performance of this classifier. It was concluded that for the AWID3 dataset and this specific classification problem, the default parameters for the RandomForestClassifier showed superior performance. Subsequently, additional experimentation was terminated at this stage. 

\begin{table}[h]
\centering
\caption{Parameters for Random Forest Classifier \parencite{scikit-learn}}
\label{tab:rf-params}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Parameter} & \textbf{Description} \\ \hline
n\_estimators & The number of trees in the forest\\
criterion & Function to measure the quality of a split.\\
max\_depth & Maximum depth of the tree. \\
min\_samples\_split & Minimum samples required to split an internal node. \\ 
min\_samples\_leaf & Minimum samples required to be at a leaf node. \\
max\_features & Maximum features to consider when splitting. \\
bootstrap & To bootstrap samples when constructing trees \\
class\_weight & Weights associated with classes \\
random\_state & The random seed.\\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{RF Model Parameters}
\label{tab:rf-parameters}
\begin{tabular}{llllll}
\hline
Parameter & Model 0-2 & Model 3 & Model 4 & Model 5\\ \hline
n\_estimators: & 100 & 100 & 200 & 100\\
max\_depth: & None & 10 & 15 & 10  \\
min\_samples\_leaf: & 1 & 2 & 1 & 2 \\
min\_samples\_split: & 2 & 3 & 2 & 3 \\
random\_state: & 1234 & 1234 & 1234 & 1234 \\
class\_weight: & None & None & balanced & balanced \\ \hline
\end{tabular}
\end{table}

