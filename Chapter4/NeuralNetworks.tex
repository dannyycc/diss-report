\subsection{Neural Networks}
\label{sec: Neural Networks}

\subsubsection{Multi-Layer Perceptron (MLP)}
\label{sec: MLP Neural Network v1}

As part of the neural network experiments, Multilayered Perceptron models were created and tested through an exploratory process. A wide range of MLP models was explored, and the selection presented in the results section is a curated list chosen for performance or notability. All models evaluated and their corresponding code can be found within the codebase. Table \ref{tab:mlp-models-1} and \ref{tab:mlp-models-2} details the parameters used for each notable MLP model.  

Experimentation began with a three-hidden layered MLP model consisting of 128, 64 and 32 neurons across the different subsets of data to gauge a rough estimate of the model's performance through varying levels of data. As such, cross-validation was not utilised. Models 0-3 consist of the exact parameters tested across the 60 and 80\% datasets; metrics were high. However, the models struggled to predict minority classes, resulting in low recall and F1-Scores. Performance when increasing the size of the dataset did improve the performance. It can be attributed to the fact the larger dataset provided more samples of the minority class to be trained on. This information was used to create further models with increased batch sizes and epochs to train for longer.

\subsubsection*{Overfitting}

A key aspect when training the MLP models was to prevent overfitting. To help mitigate this, techniques such as Early Stopping and Dropout were used in most models. Early Stopping was used during SCV; the training process monitors the validation AUC loss for signs of overfitting (e.g. when the model starts to learn the data and not generalise). The model would stop training once the validation loss began to degrade over two defined epochs. Dropout is a regularisation method that randomly sets 0.2 of the input neurons to 0. Dropout layers were used in the network architecture.

 
\subsubsection*{Activator}
Due to the nature of the problem (multi-class classification), applying existing knowledge and experience, the softmax activator was chosen for the output layer. It provides an easy-to-interpret output of the model as a list of probabilities for each class and uses the highest probability as the predicted class. 

\subsubsection*{Tuning}

The device used to train, and the experiment was the M2 Mac Mini, experiments conducted on the VM were found to be slower and would frequently cause crashes, even when utilising the dedicated GPU. As such, the hardware and time constraints restricted the level of tuning and parameter searching that could be performed. Techniques such as GridSearchCV and RandomisedSearchCV were not feasible when combined with 10 Fold S-CV. 

\subsubsection*{Thresholds}
Towards the latter stages of experimentation, further attempts were made to enhance the models' performance on the misclassified classes. The individual class weightings were adjusted using the thresholds of each class. The aim was to identify the optimal threshold level between 0-1 that would maximise the F1 score for that class. A systematic approach was followed to adjust the value in the class and evaluate the confusion matrices for prediction changes. However, this was not explored with great depth, leading the door for future work.

\medskip
Due to the complexity and computational demands of running machine learning models, practical limitations such as time constraints result in fewer tested models than desired. After conducting many experiments and achieving high-performance results, the decision was made to conclude further model experimentation. 
 
\medskip
\begin{table}[H]
\centering
\caption{MLP Model Parameters Pt 1}
\label{tab:mlp-models-1}
\begin{tabular}{llll}
\hline
Parameter & Model 0-3 & Model 4 & Model 5 \\ \hline
Asctivator: & ReLU & ReLU & ReLU \\
Output Activator: & Softmax & Softmax & Softmax \\
Initialiser: & he\_uniform & he\_uniform & he\_uniform \\
Optimiser: & Adam & Adam & SGD \\
Momentum: & N/A & N/A & N/A \\
Early Stopping: & N/A & 2 & 2 \\
Dropout: & 0.2 & 0.2 & 0.2 \\
Learning Rate: & 0.001 & 0.001 & 0.01 \\
Loss: & CC & CC & CC \\
Batch Norm: & True & True & True \\
Hidden Layers: & 3 & 3 & 4 \\
Nodes per Layer: & 128/64/32 & 128/64/32 & 256/128/64/32 \\
Batch Size: & 180 & 200 & 132 \\
Epochs: & 15 & 20 & 20 \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MLP Model Parameters Pt 2}
\label{tab:mlp-models-2}
\begin{tabular}{lll}
\hline
Parameter         & Model 6       & Model 7         \\ \hline
Activator:        & LeakyReLU     & ReLU            \\
Output Activator: & Softmax       & Softmax         \\
Initialiser:      & he\_uniform   & -               \\
Optimiser:        & Adam          & SGD             \\
Momentum:         & N/A           & 0.9             \\
Early Stopping:   & 2             & 2               \\
Dropout:          & 0.2           & 0.25*3/0.2*2    \\
Learning Rate:    & 0.01          & 0.01            \\
Loss:             & CC            & CC              \\
Batch Norm:       & True          & True            \\
Hidden Layers:    & 4             & 5               \\
Nodes per Layer:  & 256/128/64/32 & 100/80/60/40/20 \\
Batch Size:       & 132           & 170             \\
Epochs:           & 20            & 20              \\ \hline
\end{tabular}
\end{table}
