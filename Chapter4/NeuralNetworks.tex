

\subsection{Neural Networks}
\label{sec: Neural Networks}

\subsubsection{Multi-Layer Perceptron (MLP)}
\label{sec: MLP Neural Network v1}

Table \ref{tab:seq_nn} specifies the parameter values for a multi-layered feed forward neural network model, consisting of one input layer (128 neurons) and one hidden layer (64 neurons) using the ReLu activator function. The output layer has a subsequent 7 neurons corresponding to the 7 different output classes, using a soft-max activation function to produce the class probabilities. See Appendix \ref{appx: MLP NN v1} for the full code.

\begin{table}[h]
\captionsetup{justification=centering} 
\centering
\caption{MLP v1 Specifications}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\ \hline
Activator & Relu \\
Output Activator & Softmax \\
Initialiser & Default \\
Optimiser & Adam \\
Momentum & N/A \\
Early Stopping & N/A \\
Dropout & N/A \\
Learning Rate & Default \\
Loss & Categorical Crossentropy \\
Batch Norm & N/A \\
Hidden Layers & 2 \\
Nodes per Layer & 128, 64 \\
Batch Size & 32 \\
Epochs & 10 \\ \hline
\end{tabular}
\label{tab:seq_nn}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{MLP v1 Classification Report}
  \label{tab:mlp_v1_class_report}
    \begin{tabular}{lcccc}
    \toprule
    Class & Precision & Recall & F1-Score & Support \\
    \midrule
    Botnet & 0.65 & 0.53 & 0.58 & 8530 \\
    Malware & 0.83 & 0.68 & 0.75 & 19738 \\
    Normal & 0.99 & 1.00 & 0.99 & 2286103 \\
    SQL Injection & 0.98 & {\color{red}\bfseries 0.23} & 0.37 & 395 \\
    SSDP & 1.00 & 1.00 & 1.00 & 824978 \\
    SSH & {\color{red}\bfseries 0.58} & 0.38 & {\color{red}\bfseries 0.46} & 1782 \\
    Website Spoofing & 0.92 & 0.92 & 0.92 & 60766 \\
    \midrule
    Accuracy & & & 0.99 & 3202292 \\
    Macro Avg & 0.85 & 0.68 & 0.72 & 3202292 \\
    Weighted Avg & 0.99 & 0.99 & 0.99 & 3202292 \\
    \bottomrule
    \end{tabular}%
\end{table}%

After tuning, we proceeded to create an additional model with additional layers 

\begin{table}[hp]
\captionsetup{justification=centering} 
\centering
\caption{MLP v2 Specifications}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\ \hline
Activator & Relu \\
Output Activator & Softmax \\
Initialiser & he\_uniform \\
Optimiser & Adam \\
Momentum & N/A \\
Early Stopping & N/A \\
Dropout & 0.2 \\
Learning Rate & 0.001 \\
Loss & Categorical Crossentropy \\
Batch Norm & Yes \\
Hidden Layers & 3 \\
Nodes per Layer & 128, 64, 32 \\
Batch Size & 180 \\
Epochs & 15 \\ \hline
\end{tabular}
\label{tab:mlp_v2}
\end{table}

\begin{table}[hp]
\captionsetup{justification=centering} 
\centering
\caption{MLP v3 Specifications}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\ \hline
Activator & Relu \\
Output Activator & Softmax \\
Initialiser & he\_uniform \\
Optimiser & Adam \\
Momentum & N/A \\
Early Stopping & True (2 rounds) \\
Dropout & 0.2 \\
Learning Rate & 0.001 \\
Loss & Categorical Crossentropy \\
Batch Norm & Yes \\
Hidden Layers & 3 \\
Nodes per Layer & 128, 64, 32 \\
Batch Size & 200 \\
Epochs & 20 \\ \hline
\end{tabular}
\label{tab:mlp_v3}
\end{table}

\begin{table}[]
\begin{tabular}{llllll}
\hline
Parameter & Model 1 & Model 2 & Model 3 & Model 4 & Model 5 \\ \hline
Activator: & ReLU & ReLU & ReLU & ReLU & ReLU \\
Output Activator: & Softmax & Softmax & Softmax & Softmax & Softmax \\
Initialiser: & Default & he uniform & he\_uniform & he\_uniform & he uniform \\
Optimiser: & Adam & Adam & Adam & Adam & SGD \\
Momentum: & N/A & N/A & N/A & N/A & 0.9 \\
Early Stopping: & N/A & N/A & 2 Rounds & 2 Rounds & N/A \\
Dropout: & N/A & 0.2 & 0.2 & 0.2 & 0.25, 0.2 \\
Learning Rate: & Default & 0.001 & 0.001 & 0.001 & 0.01 \\
Loss: & CC & CC & CC & SCC & CC \\
Batch Norm: & N/A & Yes & Yes & Yes & Yes \\
Hidden Layers: & 2 & 3 & 3 & 3 & 4 \\
Nodes per Layer: & 128, 64 & 128, 64, 32 & 128, 64, 32 & 128, 64, 32 & 100, 80, 60, 40, 20 \\
Batch Size: & 32 & 180 & 200 & 200 & 180 \\
Epochs: & 10 & 15 & 20 & 20 & 15 \\ \hline
\end{tabular}
\end{table}