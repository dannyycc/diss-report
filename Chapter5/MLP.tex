\subsection{MLP}

In exploring Neural Networks, a series of MLP models were created with a varying number of parameters and was tested with different subsets of the dataset. Each model consisted of a different number of hidden layers and neurons, optimisers (Adam \& SGD), regularisation techniques (Dropout and Early Stopping), learning rates etc. The primary metrics for evaluating the models were AUC and F1, but the classification and confusion matrices were considered and used to form a detailed picture of each model's performance on the individual attack classes. Table \ref{tab:mlp-scv-metrics} and \ref{tab:mlp-test-metrics} show the S-CV and Test Set results for 8 MLP NN models. 

\begin{table}[h]
\centering
\caption{MLP S-CV Metrics}
\label{tab:mlp-scv-metrics}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{Model ID} & \textbf{Dataset} & \textbf{AUC} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy}  \\ \hline
4 & 100\% & 99.90 & 99.37 & 99.40 & 99.42 & 99.42 \\ \hline
5 & 100\% & {\color{red} 98.40} & {\color{red} 94.68} & {\color{red} 96.85} & {\color{red} 95.49} & {\color{red} 95.49} \\ \hline
6 & 100\% & 99.79 & 99.27 & 99.31 & 99.33 & 99.33 \\ \hline
7 & 100\% & 99.72 & 99.23 & 99.25 & 99.31 & 99.31 \\ \hline

\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MLP Test Metrics}
\label{tab:mlp-test-metrics}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{Model ID} & \textbf{Dataset} & \textbf{AUC} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy}  \\ \hline
0 & 60\% & 99.99 & 99.36 & 99.38 & 99.41 & 99.41 \\ \hline
1 & 60\% & 99.99 & 99.34 & 99.36 & 99.38 & 99.38 \\ \hline
2 & 80\% & 99.86 & 99.42 & 99.44 & 99.39 & 99.44 \\ \hline
3 & 80\% & 99.98 & 99.39 & 99.44 & 99.44 & 99.44 \\ \hline
4 & 100\% & 99.94 & 99.42 & 99.44 & 99.46 & 99.46 \\ \hline
5 & 100\% & 99.88 & 99.36 & 99.37 & 99.40 & 99.40 \\ \hline
6 & 100\% & 99.80 & 99.28 & 99.40 & 99.42 & 99.42 \\ \hline
7 & 100\% & 99.84 & 99.29 & 99.33 & 99.35 & 99.35 \\ \hline
\end{tabular}
\end{table}

\subsubsection*{Epoch \& Batch Size}

The number of epochs is the number of complete passes the model will be trained on in the dataset. When the number is too low, the model may fail to learn the data and its relationships and under fits; performing poorly on unseen data. Alternatively, when the number of epochs is too high the model may begin to overfit and memorise the training data; performing poorly on new data. The batch size defines the number of samples the model works through before the model's internal parameters are changed. % https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/

Model 4 highlights the importance of dataset size and parameters such as batch size and epochs, although the model shares similar parameters as Models 0-3, the batch size increased from 180 to 200 and 15 epochs to 20. It is important to note that although the number of epochs was increased, it was observed during CV that the model would often stop at around 10 epochs due to early stopping. The model delivered a great performance on the test set with an AUC, F1, Precision, Recall and Accuracy all around 99\%. In previous models, SQL Injection was predicted poorly, with low overall recall scores. The larger dataset and batch size helped to increase this score, but in general, the models struggle to correctly classify this class. 


\subsubsection*{Data Subsets}

Examining the results across the different data subsets, both 60\% and 80\% models showed high precision and recall. Class-specific performances for minority classes were consistently low. Notable, in model 3 on the 80\% subset there was an increase in recall within the SQL Injection class. The models exhibited high overall performance but struggled with frequent misclassifications which suggest more data is required for the model to correctly identify the specific classes.

\subsubsection*{LeakyReLU}

Models 5 and 6 differ in the selection of the activation function. (ReLU in model 5 and LeakyReLU in model 6). Upon initial examination, there are differences in test metrics, but larger differences appear when looking at class-specific performances. 
Whilst the precision was increased in some classes such as Botnet and SSH, recall suffered substantially such as 0.13 for SQL Injection, indicating the model was able to reduce some false positives at the high cost of failing to identify the true positives. 

\subsubsection*{Previous Works}
The works by \textcite{s22155633} similarly used an MLP model consisting of four hidden layers, these specifications were adopted in Model 7 to provide context. The model displayed an AUC of 99.84, F1 of 99.28, Recall of 99.35 and Accuracy of 99.35 on the test set. However, the precision, recall and F1 for SQL Injection are substantially lower at 0.02 and 0.05 compared to other classes. The model fails to identify this class accurately, similarly, Botnet and Malware also saw a drop in performance. The confusion matrix further affirms this observation with a large number of predictions from those classes being misclassified as Normal traffic. This further emphasises the importance of tuning parameters and settings that are specific to the problem at hand. 


\subsubsection*{Summary}

The Multi-Layer Perceptron in TensorFlow can provide a vast array of parameters and options, leading to an endless number of combinations to be tailored and optimised. Finding the 'best' model in the classification problem is a difficult non-trivial task. With challenges and limitations in the hardware and time, the approach for these experiments consisted of creating a sequence of models and comparing the performance metrics to previous models and the overall domain knowledge and task. Experimentation stopped after a vast number of models were tested and reached diminishing returns. Whilst this approach does not guarantee the 'best' MLP configuration, it provides a practical and effective method that strikes a balance between complexity and constraints. Further work can be investigated into utilising parameter searching such as GridSearchCV to automate the process.
