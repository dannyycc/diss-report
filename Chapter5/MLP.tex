\subsection{MLP}

In exploring Neural Networks, a series of MLP models were created with a varying number of parameters and was tested with different subsets of the dataset. Each model consisted of several hidden layers and neurons, optimisers (Adam \& SGD), regularisation techniques (Dropout and Early Stopping), learning rates etc. Table \ref{tab:mlp-scv-metrics} and \ref{tab:mlp-test-metrics} show the S-CV and Test Set results for 8 MLP NN models. 

\begin{table}[h]
\centering
\caption{MLP S-CV Metrics}
\label{tab:mlp-scv-metrics}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{Model ID} & \textbf{Dataset} & \textbf{AUC} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy}  \\ \hline
4 & 100\% & 99.90 & 99.37 & 99.40 & 99.42 & 99.42 \\ \hline
5 & 100\% & {\color{red} 98.40} & {\color{red} 94.68} & {\color{red} 96.85} & {\color{red} 95.49} & {\color{red} 95.49} \\ \hline
6 & 100\% & 99.79 & 99.27 & 99.31 & 99.33 & 99.33 \\ \hline
7 & 100\% & 99.72 & 99.23 & 99.25 & 99.31 & 99.31 \\ \hline

\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MLP Test Metrics}
\label{tab:mlp-test-metrics}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{Model ID} & \textbf{Dataset} & \textbf{AUC} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy}  \\ \hline
0 & 60\% & 99.99 & 99.36 & 99.38 & 99.41 & 99.41 \\ \hline
1 & 60\% & 99.99 & 99.34 & 99.36 & 99.38 & 99.38 \\ \hline
2 & 80\% & 99.86 & 99.42 & 99.44 & 99.39 & 99.44 \\ \hline
3 & 80\% & 99.98 & 99.39 & 99.44 & 99.44 & 99.44 \\ \hline
4 & 100\% & 99.94 & 99.42 & 99.44 & 99.46 & 99.46 \\ \hline
5 & 100\% & 99.88 & 99.36 & 99.37 & 99.40 & 99.40 \\ \hline
6 & 100\% & 99.80 & 99.28 & 99.40 & 99.42 & 99.42 \\ \hline
7 & 100\% & 99.84 & 99.29 & 99.33 & 99.35 & 99.35 \\ \hline
\end{tabular}
\end{table}

\subsubsection*{Epoch \& Batch Size}

The number of epochs is the number of complete passes the model will be trained on in the dataset. When the number is too low, the model may fail to learn the data and its relationships and under-fits, performing poorly on unseen data. Alternatively, when the number of epochs is too high, the model may overfit and memorise the training data, performing poorly on new data. The batch size defines the number of samples the model works through before the model's internal parameters are changed \parencite{brownlee_2018_difference}. % https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/

Model 4 highlights the importance of dataset size and parameters such as batch size and epochs; However, the model shares similar parameters as Models 0-3; the batch size increased from 180 to 200 and 15 epochs to 20. It is important to note that although the number of epochs was increased, it was observed during CV that the model would often stop at around ten epochs due to early stopping. The model delivered an outstanding performance on the test set with an AUC, F1, Precision, Recall and Accuracy all around 99\%. In previous models, SQL Injection was predicted poorly, with low overall recall scores. The larger dataset and batch size helped increase this score, but the models generally struggle to classify this class correctly. 


\subsubsection*{Data Subsets}

Both 60\% and 80\% models showed high precision and recall when examining the results across the different data subsets. Class-specific performances for minority classes were consistently low. Notable, in model 3 on the 80\% subset, recall increased within the SQL Injection class. The models exhibited high overall performance but struggled with frequent misclassifications which suggest more data is required for the model to correctly identify the specific classes.

\subsubsection*{LeakyReLU}

Models 5 and 6 differ in the selection of the activation function. (ReLU in model 5 and LeakyReLU in model 6). Upon initial examination, there are differences in test metrics, but larger differences appear when looking at class-specific performances. 
Whilst the precision was increased in some classes, such as Botnet and SSH, recall suffered substantially, such as 0.13 for SQL Injection, indicating the model could reduce some false positives at the high cost of failing to identify the true positives. 

\subsubsection*{Previous Works}
The works by \textcite{s22155633} similarly used an MLP model consisting of four hidden layers; these specifications were adopted again in Model 7 to provide context for comparison. Although fundamentally different, their models achieved metrics of around 75\% in AUC and 70\% in F1 across the 802.11 and Non-802.11 sets. The model in this project displayed an AUC of 99.84, F1 of 99.28, Recall of 99.35 and Accuracy of 99.35 on the test set. However, the precision, recall and F1 for SQL Injection are substantially lower at 0.02 and 0.05 compared to other classes. The model fails to identify this class accurately. Similarly, Botnet and Malware also saw a drop in performance. The confusion matrix affirms this observation, with many predictions from those classes being misclassified as Normal traffic. This further emphasises the importance of tuning parameters and settings specific to the problem at hand. 

\subsubsection*{Summary}

The Multi-Layer Perceptron in TensorFlow can provide a vast array of parameters and options, leading to endless combinations to be tailored and optimised. Finding the 'best' model in the classification problem is a difficult non-trivial task. With challenges and limitations in the hardware and time, the approach for these experiments consisted of creating a sequence of models and comparing the performance metrics to previous models and the overall domain knowledge and task. Experimentation stopped after many models were tested and reached diminishing returns. Whilst this approach does not guarantee the 'best' MLP configuration, it provides a practical and effective method that balances complexity and constraints. To automate the process, further work can be investigated into utilising parameter searching such as GridSearchCV.
